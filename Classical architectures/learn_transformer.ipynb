{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc258c9-75a6-4b84-8cb1-ea571d5fd55d",
   "metadata": {},
   "source": [
    "**Q:** I want to learn the Transformer architecture in deep learning from the scratch. However, the mathematical part in the original paper is difficult for me to understand well. Could you give me some advice?\n",
    "\n",
    "**A (by ChatGPT):** Understanding the Transformer architecture from scratch can be a challenging task, especially when dealing with the mathematical details presented in the original paper. Here are a few suggestions to help you grasp the concepts and overcome the difficulties:\n",
    "\n",
    "1. Start with the basics: Familiarize yourself with the fundamental concepts and prerequisites of deep learning, such as neural networks, attention mechanisms, and sequence modeling. Ensure you have a solid understanding of these topics before diving into the Transformer architecture.\n",
    "\n",
    "2. Break it down: Rather than trying to comprehend the entire architecture in one go, break it down into smaller components and focus on understanding each part individually. Begin with the self-attention mechanism, as it is a core component of the Transformer. Study how it calculates attention weights, computes weighted sums, and handles multiple attention heads.\n",
    "\n",
    "3. Utilize visualizations: Visualizations can be helpful in gaining an intuitive understanding of complex concepts. Look for visual explanations or diagrams that illustrate the Transformer architecture and its components. These visuals can aid in visualizing the flow of information and the interactions between different parts of the model.\n",
    "\n",
    "4. Explore online resources: Seek out alternative resources beyond the original paper. Numerous blog posts, tutorials, and videos provide explanations of the Transformer architecture in a more accessible and intuitive manner. Some popular resources include the \"Illustrated Transformer\" blog post by Jay Alammar and various YouTube tutorials on the subject.\n",
    "\n",
    "5. Study code implementations: Reading and analyzing code implementations of the Transformer architecture can provide valuable insights. Look for open-source implementations available on platforms like GitHub, and try to follow the code to understand how the components and mathematical formulas are implemented in practice.\n",
    "\n",
    "6. Seek explanations from others: Engage in discussion forums, online communities, or local study groups focused on deep learning. Pose questions, seek clarifications, and discuss the concepts with fellow learners or experts in the field. Getting different perspectives and explanations can enhance your understanding.\n",
    "\n",
    "7. Practice with hands-on exercises: Reinforce your understanding by implementing the Transformer architecture yourself. Start with simpler tasks and datasets, such as language modeling or machine translation, and gradually work your way up to more complex applications. Hands-on experience will solidify your understanding of the architecture and its mathematical formulations.\n",
    "\n",
    "Remember that understanding the Transformer architecture takes time and persistence. Be patient with yourself and focus on gradually building your knowledge and intuition. It's also important to note that mathematical rigor is not always necessary for applying and utilizing deep learning models effectively. As long as you have a good conceptual understanding and practical experience, you can leverage the Transformer architecture in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a468e47-fddb-42d7-a57b-494b37814ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/losses.py\", line 2432, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/backend.py\", line 5809, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, None, 784) vs (None, 784)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Generate reconstructed images\u001b[39;00m\n\u001b[1;32m     81\u001b[0m reconstructed_images \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test[:\u001b[38;5;241m10\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/y9/8370ybwn3zn8y1b6zp8fv7600000gn/T/__autograph_generated_fileyv2t9m81.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/losses.py\", line 2432, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/opt/miniconda3/envs/ds/lib/python3.8/site-packages/keras/src/backend.py\", line 5809, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, None, 784) vs (None, 784)).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the Self-Attention layer\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = layers.Dense(units=self.d_model)\n",
    "        self.key_dense = layers.Dense(units=self.d_model)\n",
    "        self.value_dense = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.combine_heads = layers.Dense(units=self.d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attention_logits = tf.matmul(query, key, transpose_b=True)\n",
    "        attention_logits = attention_logits / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "\n",
    "        attention_output = tf.matmul(attention_weights, value)\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape the images\n",
    "x_train = tf.reshape(x_train, shape=(x_train.shape[0], -1))\n",
    "x_test = tf.reshape(x_test, shape=(x_test.shape[0], -1))\n",
    "\n",
    "# Build the model with Self-Attention\n",
    "inputs = layers.Input(shape=(784,))\n",
    "x = SelfAttention(num_heads=8, key_dim=64)(inputs)\n",
    "outputs = layers.Dense(units=784, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=x_train,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, x_test)\n",
    ")\n",
    "\n",
    "# Generate reconstructed images\n",
    "reconstructed_images = model.predict(x_test[:10])\n",
    "\n",
    "# Visualize the original and reconstructed images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(10, 2))\n",
    "\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(x_test[i].reshape((28, 28)), cmap=\"gray\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(reconstructed_images[i].reshape((28, 28)), cmap=\"gray\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c58d5-8a1c-4676-bb7f-430fa44fd1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
